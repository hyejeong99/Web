#!/usr/bin/env python
from collections import OrderedDict

import torch, random, copy, dill, os, io, platform
import torch.nn as nn
import torch.nn.functional as F

class Qnet(nn.Module):
    def __init__(self, input_size, output_size, hidden_size, modelName):
        super(Qnet, self).__init__()

        self.model_name = modelName
        modullist = []
        modullist.append(("InputLayer", nn.Linear(input_size, hidden_size[0])))

        cnt = 0
        for layer in range(len(hidden_size)-1):
            modullist.append(("Relu_"+str(cnt), nn.ReLU()))
            modullist.append(("hiddenlayer_"+str(cnt), nn.Linear(hidden_size[layer], hidden_size[layer+1])))
            cnt += 1
  
        modullist.append(("Relu_"+str(cnt), nn.ReLU()))

        if (self.model_name == "DQN") or (self.model_name == "DDQN"):
            # DQN and DDQN
            modullist.append(("OutputLayer", nn.Linear(hidden_size[len(hidden_size)-1], output_size)))
            self.main_model = nn.Sequential(OrderedDict(modullist))
            print(self.main_model)

        elif (self.model_name == "Duel DQN"):
            # Duel DQN
            valuemodel = copy.deepcopy(modullist)
            valuemodel.append(("valOutputLayer", nn.Linear(hidden_size[len(hidden_size)-1], 1)))
            
            advantagemodel = copy.deepcopy(modullist)
            advantagemodel.append(("advOutputLayer", nn.Linear(hidden_size[len(hidden_size)-1], output_size)))
            
            self.value_model = nn.Sequential(OrderedDict(valuemodel))
            self.adv_model = nn.Sequential(OrderedDict(advantagemodel))
            print(self.value_model)
            print(self.adv_model)

    def forward(self, x):
        if (self.model_name == "DQN") or (self.model_name == "DDQN"):
            # DQN and DDQN
            x = self.main_model(x)

        elif (self.model_name == "Duel DQN"):
            #Duel DQN
            v = self.value_model(x)
            a = self.adv_model(x)
            x = v + (a - a.mean())

        return x
      
    def sample_action(self, obs, epsilon=0):
        out = self.forward(obs)
        nor = [[0, 0],[0, 1],[0, 2],[1, 0],[1, 2],[1, 3],[2, 0],[2, 1],[2, 2]]
        if (random.random() >= epsilon):
            return nor[int(out.argmax().item())]
        else: 
            return [random.randint(0, 2), random.randint(0, 2)]
    
    def get_model_name(self):
        return self.model_name

normal_list = None
def study_init(input_size, hidden_size, learning_rate, model_name, batch_size):
    global optimizer, device, output_size, q, q_target, MSE_loss, normal_list
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    output_size = 6

    q = Qnet(input_size, output_size, hidden_size, model_name).to(device)
    q_target = Qnet(input_size, output_size, hidden_size, model_name).to(device)
    q_target.load_state_dict(q.state_dict())

    print(q.parameters())
    optimizer = torch.optim.Adam(q.parameters(), lr=learning_rate)
    MSE_loss = torch.nn.MSELoss()
    return device, q, q_target, optimizer

def study_get_action(state, E_greedyFunc=0):
    global q, device
    
    rtn = q.sample_action(torch.from_numpy(state).float().to(device), E_greedyFunc)
    return rtn

def study_update():
    global q_target, q
    q_target.load_state_dict(q.state_dict())

def study_train(count, batch_size, discount_factor, replay_memory):
    global optimizer, device, output_size, q, q_target, MSE_loss
    model_name = q.get_model_name()

    for ii in range(count):
        state, action, reward, next_state, done_mask = replay_memory.sample(batch_size)
        act_tensor = torch.tensor(action)
        act_tensor = torch.reshape(act_tensor, [64, 2])
        print(act_tensor)
        angle_action = act_tensor[:,0]
        speed_action = act_tensor[:,1]
        #print(state, action, reward, next_state, done_mask)
        state = state.to(device)
        angle_action = angle_action.to(device)
        speed_action = speed_action.to(device)
        reward = reward.to(device)
        next_state = next_state.to(device)
        done_mask = done_mask.to(device)

        q_out = q(state)
        q_out = torch.reshape(q_out, [64, 2, 3])
        q_out_angle = q_out[:, 0]
        print(q_out_angle, angle_action)
        q_out_angle = q_out_angle.gather(1, angle_action)
        q_out_speed = q_out[:, 1]
        q_out_speed = q_out_speed.gather(1, speed_action)
        
        #q_out = torch.add( * 3, )
        #q_out = q(state).gather(1, action)

        if (model_name == "DQN") or (model_name == "Duel DQN"):
            #DQN and DUEL DQN
            max_q_prime_out = q_target(next_state)
            #print(max_q_prime_out, "asdf")
            max_q_prime_out = torch.reshape(max_q_prime_out, [64, 2, 3])
            max_q_prime_out = max_q_prime_out.max(2)[1]
            max_q_prime_out = torch.add(max_q_prime_out[:, 0] * 3, max_q_prime_out[:, 1])
            #print(max_q_prime_out)
            
            #print(max_q_prime_out, ii)
            #max_q_prime_speed_out = max_q_prime_out
            #print(max_q_prime_angle_out, "|", max_q_prime_speed_out)
            #max_q_prime_out = max_q_prime_out.max(1)[0].unsqueeze(1)
            #max_q_prime_angle_out = max_q_prime_out.max(1)[0].unsqueeze(1)
            #max_q_prime_speed_out = max_q_prime_out.max(1)[0].unsqueeze(1)

        elif (model_name == "DDQN"):
            #DDQN
            max_q_prime_out = q_target(next_state).detach().gather(1, q(next_state).detach().max(1)[1].unsqueeze(1))

        target = reward + discount_factor * max_q_prime_out * done_mask
        
        if (model_name == "DQN") or (model_name == "DDQN"):
            #DQN and DDQN
            loss = F.smooth_l1_loss(q_out, target.to(device)).to(device)
        elif (model_name == "Duel DQN"):
            loss = MSE_loss(q_out, target).to(device)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    return loss.data

def study_model_save(episode):
    global q
    SavePath_main = os.getcwd()+"/save/main_model_"+str(episode).zfill(6)+".pth"
    SaveBuffer = io.BytesIO()
    torch.save(q.state_dict(), SaveBuffer, pickle_module=dill)
    with open(SavePath_main, "wb") as f:
        f.write(SaveBuffer.getvalue())

def study_model_load(episode):
    global q, q_target, device
    LoadPath_main = os.getcwd()+"/save/main_model_"+str(episode).zfill(6)+".pth"
    with open(LoadPath_main, 'rb') as f:
        LoadBuffer = io.BytesIO(f.read())
    q.load_state_dict(torch.load(LoadBuffer, map_location=device))
    q_target.load_state_dict(q.state_dict())
        


